{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project II :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal Based Classifier and Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **%%capture** command supress the output of the cell. In this case it has the only purpose to keep the notebook clean. Delete it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture  \n",
    "!pip install gensim\n",
    "!pip install nonconformist\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from nonconformist.base import ClassifierAdapter\n",
    "from nonconformist.nc import ClassifierNc\n",
    "from nonconformist.icp import IcpClassifier\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import os.path\n",
    "from os import path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "\n",
    "The classifier needs a lot of parameters to generate the model and customize the predict method. \n",
    "It's posible to distinguish 4 groups of parameters: each group is characterized by the different context in which the parameters operate.\n",
    "\n",
    "#### Global Parameters:\n",
    "\n",
    "These parameters affect the classifier as a whole. They are also used in different combinations in order to test the system performance.\n",
    "\n",
    "- *significance:* the initial significance for the conformal classifier.\n",
    "- *significance_drop:* the quantity to subtract to the current significance in the case the conformal classifier returns the empty labelset.  \n",
    "- *significance_plus:* the quantity to add to the current significance in the case the conformal classifier returns the full labelset. \n",
    "- *calibration:* represent the probability of an instance of the training-set to be used for calibration. If the value is 0 then no calibration is performed and the predict will return the prediction of the base classifier at the first step.   \n",
    "- *optimization:* represent the probability of an instance of the training-set (after the removal of the calibrating set) to be used for local hyper-parameters optimization. If the value is 0 then the default values are used for the parameters and no optimization is performed.   \n",
    "\n",
    "#### Word Embedding Parameters:\n",
    "\n",
    "These parameters are linked to the gensim library which compute the word embedding. They are used to compute the embedding function necessary to transform tokens/words of an input instance in vectors of features.\n",
    "\n",
    "- *embedding_size:* the size of the embedding space (how many features per word).\n",
    "- *sg_true:* boolean parameter that allows to choose between skip-gram (=1) or CBOW (=0) as the algorithm to compute the word embedding.   \n",
    "- *window_size:* number of neighbours tokens to be considered as part of a window in the training step of the embedding process.\n",
    "- *min_count:* the number of the minimum occurrence for the tokens in order to be considered in the domain of the embedding function.\n",
    "\n",
    "#### Preprocessing Parameters:\n",
    "\n",
    "These parameters allow the customization of the preprocessing of an embedded text: every new instance must go through it in order to be processed by the predict function.  \n",
    "\n",
    "- *remove_oov:* boolean value which determines if an 'out-of-vocabulary' token shall be excluded (=0) or not (=1) wrt the list of vectors of a given instance. Keeping such token means that it will be represented by a 0 vector.   \n",
    "- *max_length:* the standard length of an instance in terms of tokens.   \n",
    "- *padding_start:* if an embedded text has less tokens then *max_length* a 0-padding is necessary, this parameter determines if the padding will be applied before (=1) or after (=0) the given list of tokens. \n",
    "- *cut_head:* if an embedded text has more tokens then *max_length* some tokens must be cut-off, this parameters determines if the cut will be made at the beggining (=1) or at the end (=0) of the given list of tokens.   \n",
    "\n",
    "#### CNN Parameters:\n",
    "\n",
    "Parameters to customize the CNN and its convolutional keras layer. \n",
    "\n",
    "- *kernel_size:* height of the convolutional filters, the width is bounded by the number of features of each embedded token.  \n",
    "- *filters:* number of filters to apply. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_PARAMETERS = {\n",
    "    'significance': 0.8,\n",
    "    'significance_drop': 0.01,\n",
    "    'significance_plus': 0.01,\n",
    "    'calibration': 0.2,\n",
    "    'optimization': 0.2,\n",
    "                    }\n",
    "\n",
    "WORD_EMBEDDING_PARAMETERS = {\n",
    "    'embedding_size': [100],\n",
    "    'sg_true': [0, 1],\n",
    "    'window_size': [5],\n",
    "    'min_count': [2]\n",
    "}\n",
    "\n",
    "PREPROCESSING_PARAMETERS = {\n",
    "    'remove_oov': [0, 1],\n",
    "    'max_length': [75, 110],\n",
    "    'padding_start': [0],\n",
    "    'cut_head': [0]\n",
    "}\n",
    "\n",
    "CNN_PARAMETERS = {\n",
    "    'kernel_size': [3, 6],\n",
    "    'filters': [32, 64],\n",
    "}\n",
    "\n",
    "# Uncomment this section for better optimization:\n",
    "# WORD_EMBEDDING_PARAMETERS = {\n",
    "#     'embedding_size': [100, 200],\n",
    "#     'sg_true': [0, 1],\n",
    "#     'window_size': [5, 7],\n",
    "#     'min_count': [2, 1]\n",
    "# }\n",
    "\n",
    "# PREPROCESSING_PARAMETERS = {\n",
    "#     'remove_oov': [0, 1],\n",
    "#     'max_length': [75, 110],\n",
    "#     'padding_start': [0, 1],\n",
    "#     'cut_head': [0, 1]\n",
    "# }\n",
    "\n",
    "# CNN_PARAMETERS = {\n",
    "#     'kernel_size': [3, 6],\n",
    "#     'filters': [32, 64],\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal Classifier Adapter:\n",
    "\n",
    "The **ConformalClassifierAdapter** class represent an interface required to work with the *nonconformist* libraries. To instantiate an object of this class the following parameters are required:  \n",
    "    - *model:* the classifier model on top of which build the wrapper  \n",
    "    - *fit_params:* additional parameters for *fit* method (default: None)  \n",
    "\n",
    "#### Methods:\n",
    "\n",
    "This wrapper class is necessary in order to instantiate a non-conformal classifier and it must implement the following methods.\n",
    "\n",
    "> **fit(self, x, y):**\n",
    "\n",
    "This function is supposed to start the training of the wrapped classifier.      \n",
    "\n",
    "**Input:**   \n",
    "    - *x:* list of instances of the training set   \n",
    "    - *y:* list of labels in OHE format  \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "\n",
    "---\n",
    "\n",
    "> **predict(self, x):**\n",
    "\n",
    "This function compute the probability vectors of the input instances wrt the classes labels.      \n",
    "\n",
    "**Input:**   \n",
    "    - *x:* list of instances to classify  \n",
    "    \n",
    "**Output:**   \n",
    "    - *prd:* list of proability-vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformalClassifierAdapter(ClassifierAdapter):\n",
    "    def __init__(self, model, fit_params=None):\n",
    "        super(ConformalClassifierAdapter, self).__init__(model, fit_params)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(x, y, epochs=20, verbose=0)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prd = self.model.predict(x)\n",
    "        return prd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Classifier:\n",
    "\n",
    "The **WorkerClassifier** class contains the implementation of the sub-classifiers of the project.   \n",
    "This object has a lot of fields in common with the **MasterClassifier** class: the only few differences are the absence of the *worker_classifiers* dictionary and the presence of the 'actual' parameters (necessary in the optimization steps) along with some other useful additional fields like *opt_loops*, *opt_counter* and *classes* used to keep track of the object state.   \n",
    "In addition to that *instances*, *labels*, *training*, *optimization* and *calibration* fields must be pre-computed in order to initialize the classifier. \n",
    "\n",
    "\n",
    "#### Methods:\n",
    "\n",
    "The class contains also the following methods.\n",
    "\n",
    "---\n",
    "\n",
    "> **compile_cnn(self, kernel, filters, embedding_size, max_length):**\n",
    "\n",
    "This function compiles the CNN given the proper parameters.      \n",
    "\n",
    "**Input:**   \n",
    "    - *kernel:* height of the filters of the convolutional layer   \n",
    "    - *filters:* number of filters   \n",
    "    - *embedding_size:* width of the filters of the convolutional layer/number of features per token    \n",
    "    - *max_length:* expected size of the instances in terms of tokens      \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "\n",
    "---\n",
    "\n",
    "> **optimize(self):**\n",
    "\n",
    "This function loop over the combinations of values of the 4 word-embedding parameters and computes, for each combination, the embedding function.      \n",
    "\n",
    "**Input:**   \n",
    "    - *None*         \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "\n",
    " ---\n",
    "\n",
    "> **optimize_preprocessing(self):**\n",
    "\n",
    "This function loop over the combinations of values of the 4 preprocessing parameters and compute, for each combination, the embedded versions of the training and optimization set.      \n",
    "\n",
    "**Input:**   \n",
    "    - *None*         \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "    \n",
    "---\n",
    "\n",
    "> **optimize_cnn(self):**\n",
    "\n",
    "This function loop over the combinations of values of the 2 convolutional parameters and compile, for each combination, the proper CNN model. This function also proceeds with the training and the evaluation phase of the neural network.        \n",
    "\n",
    "**Input:**   \n",
    "    - *None*         \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "\n",
    "---\n",
    "\n",
    "> **accuracy_check(self, candidate_accuracy):**\n",
    "\n",
    "This function update the current best parameters if the accuracy related to the last trained classifier is greater then the current best accuracy.      \n",
    "\n",
    "**Input:**   \n",
    "    - *candidate_accuracy:* accuracy of the classifier trained with the actual parameters combination.         \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "    \n",
    "---\n",
    "\n",
    "> **create_we(self, indexes, e_size, sg_true, w_size, m_count):**\n",
    "\n",
    "This function generates the embedding function thanks to gensim library and its parameters.      \n",
    "\n",
    "**Input:**   \n",
    "    - *indexes:* indexes of the instances used to generate the embedding function   \n",
    "    - *e_size:* the size of the embedding space (how many features per word)    \n",
    "    - *sg_true:* boolean parameter that allows to choose between skip-gram (=1) or CBOW (=0) as the algorithm to compute the word embedding      \n",
    "    - *w_size:* number of neighbours tokens to be considered as part of a window in the training step of the embedding process      \n",
    "    - *m_count:* number of the minimum occurrence for tokens in order to be considered in the domain of the embedding function    \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "\n",
    "---\n",
    "\n",
    "> **preprocess_raw_instance(self, single_instance, oov, pad, cut, m_length):**\n",
    "\n",
    "This function process a single instance transforming it from a list of words to a numpy array of embedded tokens.      \n",
    "\n",
    "**Input:**   \n",
    "    - *single_instance:* list of word   \n",
    "    - *oov:* boolean value which determines if an 'out-of-vocabulary' token shall be excluded (=0) or not (=1) wrt the list of vectors of a given instance. Keeping such token means that it will be represented by a 0 vector    \n",
    "    - *pad:* if the instance has less words then max_length a 0-padding is necessary, this parameter determines if the padding will be applied before (=1) or after (=0) the given list of tokens    \n",
    "    - *cut:*  if the instance has more words then max_length some tokens must be cut-off, this parameters determines if the cut will be made at the beggining (=1) or at the end (=0) of the given list of tokens  \n",
    "    - *m_length:*  the standard length of an instance in terms of tokens   \n",
    "    \n",
    "**Output:**   \n",
    "    - *r:* the preprocessed instance as numpy array  \n",
    "\n",
    "---\n",
    "\n",
    "> **predict_raw_instance(self, single_instance):**\n",
    "\n",
    "This function has the aim to predict a set of candidate classes which represent the possible labels of the input instance. The function makes use of the global parameters to be sure that the predicted set is strictly smaller the the classifier's classes set. If the calibration list is empty this function will perform a standard prediction otherwise it will perform a conformal prediction.   \n",
    "\n",
    "**Input:**   \n",
    "    - *single_instance:* single instance to classify        \n",
    "    \n",
    "**Output:**   \n",
    "    - *labels:* set of classes labels generated by the classifier or single label if *calibration* is empty \n",
    "    \n",
    "---\n",
    "\n",
    "> **train(self):**\n",
    "\n",
    "This function perform the training of the worker classifier. This step includes: the optimization phase; the generation of the words embedding function; the preprocessing of the training and calibration set; the training of the classifier. Some of these phases could be skipped according to *global_params* internal values.  \n",
    "\n",
    "**Input:**   \n",
    "    - *None*         \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerClassifier:\n",
    "\n",
    "    def __init__(self, \n",
    "                 instances,\n",
    "                 labels,\n",
    "                 training,\n",
    "                 optimization,\n",
    "                 calibration,\n",
    "                 global_params = GLOBAL_PARAMETERS,\n",
    "                 we_params = WORD_EMBEDDING_PARAMETERS,\n",
    "                 pre_params = PREPROCESSING_PARAMETERS, \n",
    "                 cnn_params = CNN_PARAMETERS):\n",
    "        \n",
    "        self.instances = instances\n",
    "        self.labels = labels\n",
    "            \n",
    "        self.training = training\n",
    "        self.optimization =  optimization\n",
    "        self.calibration = calibration\n",
    "        \n",
    "        self.global_params = global_params\n",
    "        self.we_params = we_params\n",
    "        self.pre_params = pre_params\n",
    "        self.cnn_params = cnn_params\n",
    "        \n",
    "        self.best_we_params  = {\n",
    "            'embedding_size': we_params['embedding_size'][0],\n",
    "            'sg_true': we_params['sg_true'][0],\n",
    "            'window_size': we_params['window_size'][0],\n",
    "            'min_count': we_params['min_count'][0]\n",
    "        }\n",
    "        self.best_pre_params = {\n",
    "            'remove_oov': pre_params['remove_oov'][0],\n",
    "            'padding_start': pre_params['padding_start'][0],\n",
    "            'cut_head': pre_params['cut_head'][0],\n",
    "            'max_length': pre_params['max_length'][0]\n",
    "        }\n",
    "        self.best_cnn_params = {\n",
    "            'kernel_size': cnn_params['kernel_size'][0],\n",
    "            'filters': cnn_params['filters'][0]\n",
    "        }\n",
    "        \n",
    "        #optimization loop variables\n",
    "        self.best_accuracy = -1\n",
    "        self.preprocessed_training = None\n",
    "        self.preprocessed_optimization = None\n",
    "        self.preprocessed_calibration = None\n",
    "        \n",
    "        self.current_embedding_size = None\n",
    "        self.current_sg_true = None\n",
    "        self.current_window_size = None\n",
    "        self.current_min_count = None\n",
    "        \n",
    "        self.current_remove_oov = None\n",
    "        self.current_padding_start = None\n",
    "        self.current_cut_head = None\n",
    "        self.current_max_length = None\n",
    "        \n",
    "        self.current_kernel_size = None\n",
    "        self.current_filters = None        \n",
    "                \n",
    "        self.embeddings = None\n",
    "        self.classifier = None\n",
    "        self.nc_classifier = None\n",
    "        \n",
    "        self.opt_counter = 0\n",
    "        self.opt_loops = len(we_params['embedding_size']) * len(we_params['sg_true']) * len(we_params['window_size']) * len(we_params['min_count'])\\\n",
    "                    * len(pre_params['remove_oov']) * len(pre_params['padding_start']) * len(pre_params['cut_head']) * len(pre_params['max_length'])\\\n",
    "                    * len(cnn_params['kernel_size']) * len(cnn_params['filters'])\n",
    "        self.classes = self.labels[self.training].apply(lambda x: np.argmax(x)).unique()\n",
    "        self.classes.sort()\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        print('\\nTRAINING WORKER CLASSIFIER:\\nlabels ->', self.classes)\n",
    "        # optimization step\n",
    "        if len(self.optimization) != 0:\n",
    "            self.optimize()\n",
    "            \n",
    "        # word embedding\n",
    "        print('Best accuracy: ', self.best_accuracy)\n",
    "        print('Best parameters : \\n', self.best_we_params,'\\n',self.best_pre_params,'\\n',self.best_cnn_params)     \n",
    "        \n",
    "        indexes = self.training + self.optimization\n",
    "        self.create_we(indexes,\n",
    "                       self.best_we_params['embedding_size'],\n",
    "                       self.best_we_params['sg_true'],\n",
    "                       self.best_we_params['window_size'],\n",
    "                       self.best_we_params['min_count'])\n",
    "        \n",
    "        # preprocessing\n",
    "        self.preprocessed_training = []\n",
    "        for idx in indexes:\n",
    "            preprocessed_instance = self.preprocess_raw_instance(self.instances[idx],\n",
    "                                                                 self.best_pre_params['remove_oov'],\n",
    "                                                                 self.best_pre_params['padding_start'],\n",
    "                                                                 self.best_pre_params['cut_head'],\n",
    "                                                                 self.best_pre_params['max_length'])\n",
    "            self.preprocessed_training.append(preprocessed_instance)\n",
    "        # NB: same field used in the optimization step but now wrt training+optimization sets\n",
    "        self.preprocessed_training = np.stack(self.preprocessed_training)\n",
    "\n",
    "        if len(self.calibration) != 0:\n",
    "            self.preprocessed_calibration = []\n",
    "            for idx in self.calibration:\n",
    "                preprocessed_instance = self.preprocess_raw_instance(self.instances[idx],\n",
    "                                                                     self.best_pre_params['remove_oov'],\n",
    "                                                                     self.best_pre_params['padding_start'],\n",
    "                                                                     self.best_pre_params['cut_head'],\n",
    "                                                                     self.best_pre_params['max_length'])\n",
    "                self.preprocessed_calibration.append(preprocessed_instance)\n",
    "            self.preprocessed_calibration = np.stack(self.preprocessed_calibration)\n",
    "        \n",
    "        \n",
    "        # CNN training\n",
    "        self.compile_cnn(self.best_cnn_params['kernel_size'],\n",
    "                         self.best_cnn_params['filters'],\n",
    "                         self.best_we_params['embedding_size'],\n",
    "                         self.best_pre_params['max_length'])\n",
    "        \n",
    "        tmp_tr = np.stack(np.take(self.labels, indexes).values)\n",
    "        \n",
    "        if len(self.calibration) != 0:\n",
    "            tmp_clb = np.stack(np.take(self.labels, self.calibration).values)\n",
    "            print(\"CONFORMAL TRAINING...\")\n",
    "            model = ConformalClassifierAdapter(self.classifier)\n",
    "            nc = ClassifierNc(model)\n",
    "            self.nc_classifier = IcpClassifier(nc)\n",
    "            self.nc_classifier.fit(self.preprocessed_training, tmp_tr)\n",
    "            self.nc_classifier.calibrate(self.preprocessed_calibration, np.array([np.where(r==1)[0][0] for r in tmp_clb]))\n",
    "        else:\n",
    "            print(\"STANDARD TRAINING...\")\n",
    "            self.classifier.fit(self.preprocessed_training, tmp_tr, epochs=20, verbose=0)\n",
    "        \n",
    "        print('DONE!\\n\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compile_cnn(self, kernel, filters, embedding_size, max_length):\n",
    "        \n",
    "        loss_function = 'categorical_crossentropy'\n",
    "        activation = 'softmax'\n",
    "        classifications = len(self.labels[0])\n",
    "        #print(classifications)\n",
    "        i = Input(shape=(max_length, embedding_size))\n",
    "        x = Conv1D(filters, kernel, activation='relu')(i)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dense(classifications, activation=activation)(x)\n",
    "        self.classifier = Model(i,x)\n",
    "        self.classifier.compile(loss=loss_function, optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "    def optimize(self):\n",
    "        \n",
    "        self.opt_counter = 0\n",
    "        for e_size in self.we_params[\"embedding_size\"]:\n",
    "            self.current_embedding_size = e_size\n",
    "            for sg_true in self.we_params[\"sg_true\"]:\n",
    "                self.current_sg_true = sg_true\n",
    "                for w_size in self.we_params[\"window_size\"]:\n",
    "                    self.current_window_size = w_size\n",
    "                    for m_count in self.we_params[\"min_count\"]:\n",
    "                        self.current_min_count = m_count\n",
    "                        \n",
    "                        self.create_we(self.training, e_size, sg_true, w_size, m_count)\n",
    "                        self.optimize_preprocessing()\n",
    "                    \n",
    "    \n",
    "    def optimize_preprocessing(self):\n",
    "    \n",
    "        for oov in self.pre_params[\"remove_oov\"]:\n",
    "            self.current_remove_oov = oov\n",
    "            for pad in self.pre_params[\"padding_start\"]:\n",
    "                self.current_padding_start = pad\n",
    "                for cut in self.pre_params[\"cut_head\"]:\n",
    "                    self.current_cut_head = cut\n",
    "                    for m_length in self.pre_params[\"max_length\"]:\n",
    "                        self.current_max_length = m_length \n",
    "                        \n",
    "                        self.preprocessed_training = []\n",
    "                        for idx in self.training:\n",
    "                            preprocessed_instance = self.preprocess_raw_instance(self.instances[idx], oov, pad, cut, m_length)\n",
    "                            self.preprocessed_training.append(preprocessed_instance)\n",
    "                        self.preprocessed_training = np.stack(self.preprocessed_training)\n",
    "                        \n",
    "                        self.preprocessed_optimization = []    \n",
    "                        for idx in self.optimization:\n",
    "                            preprocessed_instance = self.preprocess_raw_instance(self.instances[idx], oov, pad, cut, m_length)\n",
    "                            self.preprocessed_optimization.append(preprocessed_instance)\n",
    "                        self.preprocessed_optimization = np.stack(self.preprocessed_optimization)\n",
    "                                          \n",
    "                        \n",
    "                        self.optimize_cnn()\n",
    "                        \n",
    "                        \n",
    "    def optimize_cnn(self):\n",
    "        \n",
    "        for k in self.cnn_params[\"kernel_size\"]:\n",
    "            self.current_kernel_size = k\n",
    "            for f in self.cnn_params[\"filters\"]:\n",
    "                self.current_filters = f\n",
    "                self.opt_counter = self.opt_counter + 1\n",
    "                print(\"Tested Combinations: \", self.opt_counter, '/',self.opt_loops, end=\"\\r\")\n",
    "                \n",
    "                self.compile_cnn(k,f,self.current_embedding_size, self.current_max_length)\n",
    "                tmp_tr = np.stack(np.take(self.labels, self.training).values)\n",
    "                tmp_opt = np.stack(np.take(self.labels, self.optimization).values) \n",
    "                self.classifier.fit(self.preprocessed_training, tmp_tr, epochs=10, verbose=0)\n",
    "                new_accuracy = self.classifier.evaluate(self.preprocessed_optimization,tmp_opt, verbose=0)[1] # accuracy is stored in the second cell\n",
    "                \n",
    "                self.accuracy_check(new_accuracy)\n",
    "                \n",
    "                \n",
    "    \n",
    "    def accuracy_check(self, candidate_accuracy):\n",
    "        \n",
    "        if self.best_accuracy < candidate_accuracy: \n",
    "            \n",
    "            self.best_accuracy = candidate_accuracy\n",
    "            \n",
    "            self.best_we_params[\"embedding_size\"] = self.current_embedding_size\n",
    "            self.best_we_params[\"sg_true\"] = self.current_sg_true\n",
    "            self.best_we_params[\"window_size\"] = self.current_window_size\n",
    "            self.best_we_params[\"min_count\"] = self.current_min_count\n",
    "            \n",
    "            self.best_pre_params[\"remove_oov\"] = self.current_remove_oov\n",
    "            self.best_pre_params[\"padding_start\"] = self.current_padding_start\n",
    "            self.best_pre_params[\"cut_head\"] = self.current_cut_head\n",
    "            self.best_pre_params[\"max_length\"] = self.current_max_length\n",
    "            \n",
    "            self.best_cnn_params[\"kernel_size\"] = self.current_kernel_size \n",
    "            self.best_cnn_params[\"filters\"] = self.current_filters\n",
    "            \n",
    "            #print('\\nNew best accuracy: ', self.best_accuracy)\n",
    "            #print('\\nNew best parameters : \\n', self.best_we_params,'\\n',self.best_pre_params,'\\n',self.best_cnn_params,'\\n')\n",
    "            #print('\\n')\n",
    "    \n",
    "    #preprocessing functions Training and Optimization\n",
    "          \n",
    "    \n",
    "    def create_we(self, indexes, e_size, sg_true, w_size, m_count ):\n",
    "        self.embeddings = gensim.models.Word2Vec(\n",
    "            np.take(self.instances,indexes),\n",
    "            window=w_size,\n",
    "            size=e_size, \n",
    "            sg=sg_true, \n",
    "            min_count=m_count\n",
    "        )\n",
    "                \n",
    "    \n",
    "    #preprocessing instance (predict/train phase)\n",
    "    \n",
    "    def preprocess_raw_instance(self, single_instance, oov, pad, cut, m_length): \n",
    "        \n",
    "        l = len(self.embeddings.wv[self.embeddings.wv.index2word[0]])\n",
    "        r = []\n",
    "        \n",
    "        # embedding function application\n",
    "        for tkn in single_instance:\n",
    "            if tkn in self.embeddings.wv.index2word:\n",
    "                r.append(self.embeddings.wv[tkn])\n",
    "            else:\n",
    "                if oov == 0:\n",
    "                    r.append(np.zeros(l))\n",
    "        \n",
    "        lr = len(r)\n",
    "        \n",
    "        # padding\n",
    "        if lr < m_length:\n",
    "            tmp = [] \n",
    "            for i in range(0, m_length-lr):\n",
    "                tmp.append(np.zeros(l))\n",
    "            if pad == 0:\n",
    "                r = tmp + r \n",
    "            else:\n",
    "                r = r + tmp \n",
    "        #cut\n",
    "        if lr > m_length:\n",
    "            if cut == 0:\n",
    "                r = r[0:m_length]\n",
    "            else:\n",
    "                r = r[lr-m_length:lr]\n",
    "        \n",
    "        return np.asarray(r)\n",
    "    \n",
    "    \n",
    "    def predict_raw_instance(self, single_instance):\n",
    "        \n",
    "        labels = []\n",
    "        si = []\n",
    "        l = len(self.classes)\n",
    "        tmp_sign = self.global_params['significance']\n",
    "        si.append(self.preprocess_raw_instance(single_instance,\n",
    "                                     self.best_pre_params['remove_oov'],\n",
    "                                     self.best_pre_params['padding_start'],\n",
    "                                     self.best_pre_params['cut_head'],\n",
    "                                     self.best_pre_params['max_length']))\n",
    "        si = np.stack(si)\n",
    "        \n",
    "        if len(self.calibration) == 0:\n",
    "            labels.append(np.argmax(self.classifier.predict(si)))\n",
    "        else:\n",
    "            while len(labels) in [0, l] and len(labels) != 1:\n",
    "                tmp = self.nc_classifier.predict(si, significance = tmp_sign)\n",
    "                labels = [np.where(r==1)[0] for r in tmp][0].tolist()\n",
    "                if len(labels) == l:\n",
    "                    tmp_sign = tmp_sign + self.global_params['significance_plus']\n",
    "                if len(labels) == 0:\n",
    "                    tmp_sign = tmp_sign - self.global_params['significance_drop']\n",
    "                    \n",
    "        return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master Classifier:\n",
    "\n",
    "The **MasterClassifier** class contains the implementation of the main classifier of the project.   \n",
    "This object has several fields: some of them are hyperparameters of the classifier that are taken as input in the initialization phase. Their function is better explained in the proper section above.  \n",
    "The other fields are listed in the following lines:   \n",
    "    - *instances = None:* the list of the dataset instances that are used to generate the model     \n",
    "    - *labels = None:* the list of class labels associated to each instance of the previous list     \n",
    "    - *training = []:* the training-set instance indexes wrt the list instances      \n",
    "    - *optimization = []:* the optimization-set instance indexes wrt the list instances     \n",
    "    - *calibration = []:* the calibration-set instance indexes wrt the list instances    \n",
    "    - *worker_classifiers = {}:* the dictionary of all the trained classifiers characterized by the different set of classes they recognize \n",
    "    \n",
    "#### Methods:\n",
    "\n",
    "The class contains also the following methods.\n",
    "\n",
    "---\n",
    "\n",
    "> **load_dataset(self, instances, labels):**\n",
    "\n",
    "This function has the goal to load a dataset and split it according to the probability expressed by the global parameters proper of the classifier. The splitted datasets are stored in the *training*, *optimization*, and *calibration* fields of the classifier.      \n",
    "\n",
    "**Input:**   \n",
    "    - *instances:* instances of the dataset to load         \n",
    "    - *labels:* labels of the classes related to the dataset's instances          \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "\n",
    "---\n",
    "\n",
    "> **train(self):**\n",
    "\n",
    "The train function simply start the training of the classifier calling the **train_worker** method on all the classes labels.      \n",
    "\n",
    "**Input:**   \n",
    "    - *None*           \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*   \n",
    "    \n",
    "---\n",
    "\n",
    "> **train_worker(self, selected_labels):**\n",
    "\n",
    "This function generate and train a worker classifier wrt the list of classes listed in *selected_labels*.    \n",
    "\n",
    "**Input:**   \n",
    "    - *selected_labels:* list of the encoded class labels to train on         \n",
    "    \n",
    "**Output:**   \n",
    "    - *None*  \n",
    "    \n",
    "---\n",
    "\n",
    "> **filter_dataset(self, indexes, selected_labels):**\n",
    "\n",
    "This function has the aim to filter the input list of indexes keeping only the ones that are associated to an instance of the main dataset with label equal to one of the labels encoded in the *selected_labels* list.       \n",
    "\n",
    "**Input:**   \n",
    "    - *indexes:* list of indexes related to the main dataset        \n",
    "    - *selected_labels:* list of valid labels: each different vector-label is represented by the index of his cell which contains '1'           \n",
    "    \n",
    "**Output:**   \n",
    "    - *r:* filtered list of indexes  \n",
    "\n",
    "---\n",
    "\n",
    "> **predict_instance(self, instance):**\n",
    "\n",
    "This function keeps triggering the predict method of the internal worker classifiers until one of these calls return a single label. The worker classifiers are chosen depending on the list of labels returned by the last called predict method: if the *classifiers* dictionary doesn't contain such classifier it's immediately generated. The procedure ensure the returning of a single value because of its loop and the property of down-closure of the predict method of the worker classifiers.     \n",
    "\n",
    "**Input:**   \n",
    "    - *instance:* the single instance to classify            \n",
    "    \n",
    "**Output:**   \n",
    "    - *label[0]:* single label predicted \n",
    "\n",
    "---\n",
    "\n",
    "> **predict(self, instances):**\n",
    "\n",
    "This function tries to guess the correct class of a list of input instances returning, for each of them, a single label.       \n",
    "\n",
    "**Input:**   \n",
    "    - *istances:* either a pandas *Series* (of instances) or a single instance to classify           \n",
    "    \n",
    "**Output:**   \n",
    "    - *labels:* the list of predicted labels with cardinality equal to the input parameter *instances* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterClassifier:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 global_params=GLOBAL_PARAMETERS, \n",
    "                 we_params=WORD_EMBEDDING_PARAMETERS,\n",
    "                 pre_params=PREPROCESSING_PARAMETERS, \n",
    "                 cnn_params=CNN_PARAMETERS):\n",
    "        \n",
    "        self.global_params = global_params\n",
    "        self.we_params = we_params\n",
    "        self.pre_params = pre_params\n",
    "        self.cnn_params = cnn_params\n",
    "            \n",
    "        self.instances = None\n",
    "        self.labels = None\n",
    "            \n",
    "        self.training = []\n",
    "        self.optimization = []\n",
    "        self.calibration = []\n",
    "            \n",
    "        self.worker_classifiers = {}\n",
    "    \n",
    "    \n",
    "    def load_dataset(self, instances, labels):\n",
    "            \n",
    "        self.instances = instances\n",
    "        self.labels = labels\n",
    "        self.global_params['optimization']\n",
    "            \n",
    "        for i in range (0, len(self.instances)):\n",
    "            if random.uniform(0,1) < self.global_params['calibration']:\n",
    "                self.calibration.append(i)\n",
    "            else: \n",
    "                if random.uniform(0,1) < self.global_params['optimization']:\n",
    "                    self.optimization.append(i)\n",
    "                else: \n",
    "                    self.training.append(i)\n",
    "                            \n",
    "                            \n",
    "    def train(self):\n",
    "        labels = [i for i in range(0,len(self.labels[0]))]\n",
    "        print('\\n\\nTRAINING MASTER CLASSIFIER: \\nglobal params ->', self.global_params, '\\n')\n",
    "        self.train_worker(labels)\n",
    "    \n",
    "    \n",
    "    def train_worker(self, selected_labels):\n",
    "            \n",
    "        selected_labels.sort()\n",
    "        self.worker_classifiers[str(selected_labels)] = WorkerClassifier(\n",
    "                    self.instances,\n",
    "                    self.labels,\n",
    "                    self.filter_dataset(self.training, selected_labels),\n",
    "                    self.filter_dataset(self.optimization, selected_labels),\n",
    "                    self.filter_dataset(self.calibration, selected_labels),\n",
    "                    self.global_params,\n",
    "                    self.we_params,\n",
    "                    self.pre_params,\n",
    "                    self.cnn_params\n",
    "                )\n",
    "        self.worker_classifiers[str(selected_labels)].train()\n",
    "    \n",
    "    \n",
    "    def filter_dataset(self, indexes, selected_labels):\n",
    "        \n",
    "        r = []\n",
    "        for idx in indexes:\n",
    "            if np.take(self.labels[idx], selected_labels).mean() > 0:\n",
    "                r.append(idx)\n",
    "        return r\n",
    "\n",
    "    \n",
    "    def predict_instance(self, instance):\n",
    "        \n",
    "        lbls = [i for i in range(0,len(self.labels[0]))]\n",
    "        label = []\n",
    "        \n",
    "        while len(label) != 1:\n",
    "            if str(lbls) in self.worker_classifiers:\n",
    "                label = np.take(lbls, self.worker_classifiers[str(lbls)].predict_raw_instance(instance))\n",
    "                label.sort()\n",
    "                lbls = label \n",
    "            else:\n",
    "                self.train_worker(lbls)\n",
    "        \n",
    "        return label[0]\n",
    "    \n",
    "    \n",
    "    def predict(self, instances):\n",
    "        \n",
    "        labels = []\n",
    "        if type(instances) == pd.core.series.Series:\n",
    "            idx = 0\n",
    "            for i in instances:\n",
    "                idx += 1\n",
    "                print('Tested Instances: ', idx, '/', len(instances), end=\"\\r\")\n",
    "                labels.append(self.predict_instance(i))\n",
    "        else:\n",
    "            labels.append(self.predict_instance(instances))\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **to_multiclass(idx, classes):**\n",
    "\n",
    "This function encode the label of a class (originally expresed as a number) in a list of zeros except for the cell with index equal to the class label. Basically it's the application of the OHE on the class label values. This format is useful because it makes the computation of the softmax function  easier.      \n",
    "\n",
    "**Input:**   \n",
    "    - *idx:* current class id-number         \n",
    "    - *classes:* number of different classes          \n",
    "    \n",
    "**Output:**   \n",
    "    - *r:* list representing the encoded class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_multiclass(idx, classes):\n",
    "    r = np.zeros(classes)\n",
    "    r[idx] =  1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **chunks(lst, n):**\n",
    "\n",
    "This yield successive n-sized chunks from a list.    \n",
    "\n",
    "**Input:**   \n",
    "    - *lst:* list         \n",
    "    - *n:* size of a chunk             \n",
    "    \n",
    "**Output:**   \n",
    "    - *lst[i:i + n]:* chunk of the original list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prepocessing and Model Generation:\n",
    "\n",
    "The original dataset of this project is represented by a list of textual obfuscated instances (one instance for each line in 'xtrain_obfuscated.txt') related to 12 different novels. The classes labels are expressed as integer in the range [0-11] and they are listed in the 'ytrain.txt' file.    \n",
    "The following code loads the dataset and applies the OHE to the classes labels. Then the dataset is sampled to reduce the computational load according to the parameter *SAMPLE_SIZE*: the sampled dataset is also splitted in test and training set according to *TRAIN_SIZE*. The instances of the resulting dataset are splitted in list of tokens of length equal to the parameter *CHUNK_SIZE*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_features = pd.read_csv(\"xtrain_obfuscated.txt\", header=None, names=[\"text\"])\n",
    "ds_classes = pd.read_csv(\"ytrain.txt\", header=None, names=[\"novel\"])\n",
    "\n",
    "\n",
    "# reset_index(drop=True): reset the index of the DataFrame \n",
    "ds = pd.concat([ds_features.reset_index(drop=True),ds_classes.reset_index(drop=True)], axis=1)\n",
    "number_of_classes = len(set(ds.novel))\n",
    "ds[\"novel\"] = ds[\"novel\"].apply(lambda value: to_multiclass(value,number_of_classes )) \n",
    "\n",
    "\n",
    "TRAIN_SIZE = 0.99\n",
    "SAMPLE_SIZE = 0.5\n",
    "CHUNK_SIZE = 4\n",
    "\n",
    "\n",
    "ds[\"text\"] = ds[\"text\"].apply(lambda text:  [''.join(tokens) for tokens in chunks(list(text), CHUNK_SIZE)])\n",
    "tmp = ds.sample(frac=SAMPLE_SIZE)\n",
    "while tmp['novel'].apply(lambda x: np.argmax(x)).nunique() != 12:\n",
    "    tmp = ds.sample(frac=SAMPLE_SIZE)\n",
    "ds = tmp\n",
    "\n",
    "\n",
    "if path.exists(\"train_smpl\"+ str(SAMPLE_SIZE) +\".csv\") and path.exists(\"test_smpl\"+ str(SAMPLE_SIZE) +\".csv\"):\n",
    "    \n",
    "    train = pd.read_csv('train_smpl'+ str(SAMPLE_SIZE) +'.csv')\n",
    "    train['text'] = train['text'].apply(lambda x: x.replace(\"', '\", \" \")).apply(lambda x: x.replace(\"['\", \"\")).apply(lambda x: x.replace(\"']\", \"\")).apply(lambda x: x.split(\" \"))\n",
    "    train['novel'] = train['novel'].apply(lambda x: x.replace(\".,\", \"\")).apply(lambda x: x.replace(\"[\", \"\")).apply(lambda x: x.replace(\"]\", \"\")).apply(lambda x: x.split(\" \")).apply(lambda x: np.array(x).astype(np.float))\n",
    "\n",
    "    test = pd.read_csv('test_smpl'+ str(SAMPLE_SIZE) +'.csv')\n",
    "    test['text'] = test['text'].apply(lambda x: x.replace(\"', '\", \" \")).apply(lambda x: x.replace(\"['\", \"\")).apply(lambda x: x.replace(\"']\", \"\")).apply(lambda x: x.split(\" \"))\n",
    "    test['novel'] = test['novel'].apply(lambda x: x.replace(\".,\", \"\")).apply(lambda x: x.replace(\"[\", \"\")).apply(lambda x: x.replace(\"]\", \"\")).apply(lambda x: x.split(\" \")).apply(lambda x: np.array(x).astype(np.float))\n",
    "\n",
    "else:    \n",
    "    \n",
    "    train = ds.sample(frac=TRAIN_SIZE, random_state=0)\n",
    "    test = ds.drop(train.index)\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    # to be sure that all classes will be considered\n",
    "    while train['novel'].apply(lambda x: np.argmax(x)).nunique() != 12:\n",
    "        train = ds.sample(frac=TRAIN_SIZE, random_state=0)\n",
    "        test = ds.drop(train.index)\n",
    "        train = train.reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "    \n",
    "    train.to_csv(r'train_smpl'+ str(SAMPLE_SIZE) +'.csv', index = False, header=True)\n",
    "    test.to_csv(r'test_smpl'+ str(SAMPLE_SIZE) +'.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary stores the list of values related to the different global parameters. Combination of these values will be used to generate and test different classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_params = {\n",
    "    'significance': [0.8, 0.4],\n",
    "    'significance_drop': [0.01],\n",
    "    'significance_plus': [0.01],\n",
    "    'calibration': [0.0, 0.2],\n",
    "    'optimization': [0.0, 0.2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Step:\n",
    "\n",
    "This cell contains the code which generates and tests each classifier. The test results will be stored in the proper DataFrame *results* along with the correct class-labels of the tested instances: the resulting object is saved at each iteration as a '.csv' file with name 'res.csv'. At the same time, the dictionary *classifiers* is filled with the new classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glps = []\n",
    "keys = []\n",
    "\n",
    "if path.exists(\"res\"+ str(SAMPLE_SIZE) +\".csv\"):\n",
    "    results = pd.read_csv(\"res\"+ str(SAMPLE_SIZE) +\".csv\")\n",
    "    if len(results) != len(test):\n",
    "        results = pd.DataFrame()\n",
    "else:    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "results['ground_truth']=test['novel'].apply(lambda x: np.argmax(x))\n",
    "classifiers = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TRAINING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.0, 'optimization': 0.0} \n",
      "\n",
      "\n",
      "TRAINING WORKER CLASSIFIER:\n",
      "labels -> [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Best accuracy:  -1\n",
      "Best parameters : \n",
      " {'embedding_size': 100, 'sg_true': 0, 'window_size': 5, 'min_count': 2} \n",
      " {'remove_oov': 0, 'padding_start': 0, 'cut_head': 0, 'max_length': 75} \n",
      " {'kernel_size': 3, 'filters': 32}\n",
      "STANDARD TRAINING...\n",
      "DONE!\n",
      "\n",
      "\n",
      "TESTING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.0, 'optimization': 0.0} \n",
      "\n",
      "Tested Instances:  65 / 65\n",
      "--------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "TRAINING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.0, 'optimization': 0.2} \n",
      "\n",
      "\n",
      "TRAINING WORKER CLASSIFIER:\n",
      "labels -> [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Best accuracy:  0.40366971492767334\n",
      "Best parameters : \n",
      " {'embedding_size': 100, 'sg_true': 1, 'window_size': 5, 'min_count': 2} \n",
      " {'remove_oov': 0, 'padding_start': 0, 'cut_head': 0, 'max_length': 110} \n",
      " {'kernel_size': 6, 'filters': 64}\n",
      "STANDARD TRAINING...\n",
      "DONE!\n",
      "\n",
      "\n",
      "TESTING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.0, 'optimization': 0.2} \n",
      "\n",
      "Tested Instances:  65 / 65\n",
      "--------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "TRAINING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.2, 'optimization': 0.0} \n",
      "\n",
      "\n",
      "TRAINING WORKER CLASSIFIER:\n",
      "labels -> [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Best accuracy:  -1\n",
      "Best parameters : \n",
      " {'embedding_size': 100, 'sg_true': 0, 'window_size': 5, 'min_count': 2} \n",
      " {'remove_oov': 0, 'padding_start': 0, 'cut_head': 0, 'max_length': 75} \n",
      " {'kernel_size': 3, 'filters': 32}\n",
      "CONFORMAL TRAINING...\n",
      "DONE!\n",
      "\n",
      "\n",
      "TESTING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.2, 'optimization': 0.0} \n",
      "\n",
      "Tested Instances:  30 / 65\n",
      "TRAINING WORKER CLASSIFIER:\n",
      "labels -> [5 7]\n",
      "Best accuracy:  -1\n",
      "Best parameters : \n",
      " {'embedding_size': 100, 'sg_true': 0, 'window_size': 5, 'min_count': 2} \n",
      " {'remove_oov': 0, 'padding_start': 0, 'cut_head': 0, 'max_length': 75} \n",
      " {'kernel_size': 3, 'filters': 32}\n",
      "CONFORMAL TRAINING...\n",
      "DONE!\n",
      "\n",
      "\n",
      "Tested Instances:  65 / 65\n",
      "--------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "TRAINING MASTER CLASSIFIER: \n",
      "global params -> {'significance': 0.8, 'significance_drop': 0.01, 'significance_plus': 0.01, 'calibration': 0.2, 'optimization': 0.2} \n",
      "\n",
      "\n",
      "TRAINING WORKER CLASSIFIER:\n",
      "labels -> [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Tested Combinations:  25 / 32\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b4af24916b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'novel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TESTING MASTER CLASSIFIER: \\nglobal params ->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f0bebeac94b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nTRAINING MASTER CLASSIFIER: \\nglobal params ->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f0bebeac94b1>\u001b[0m in \u001b[0;36mtrain_worker\u001b[0;34m(self, selected_labels)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 )\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-95525ee45990>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-95525ee45990>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_we\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-95525ee45990>\u001b[0m in \u001b[0;36moptimize_preprocessing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-95525ee45990>\u001b[0m in \u001b[0;36moptimize_cnn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mtmp_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mtmp_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0mnew_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_optimization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# accuracy is stored in the second cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sig in gl_params['significance']:\n",
    "    for sig_d in gl_params['significance_drop']:\n",
    "        for sig_p in gl_params['significance_plus']:\n",
    "            for cal in gl_params['calibration']:\n",
    "                for opt in gl_params['optimization']:\n",
    "                    glp = {}\n",
    "                    glp['significance'] = sig\n",
    "                    glp['significance_drop'] = sig_d\n",
    "                    glp['significance_plus'] = sig_p\n",
    "                    glp['calibration'] = cal\n",
    "                    glp['optimization'] = opt\n",
    "                    glps.append(glp)\n",
    "                    keys.append(str([sig, sig_d, sig_p, cal, opt]))\n",
    "    \n",
    "for indx in range(len(list(results))-1, len(keys)):    \n",
    "    \n",
    "    mc = MasterClassifier(global_params=glps[indx])    \n",
    "    classifiers[keys[indx]] = [mc]\n",
    "    classifiers[keys[indx]][0].load_dataset(train['text'], train['novel'])\n",
    "    classifiers[keys[indx]][0].train()\n",
    "    \n",
    "    print('TESTING MASTER CLASSIFIER: \\nglobal params ->', glps[indx], '\\n')\n",
    "    results[keys[indx]] = classifiers[keys[indx]][0].predict(test['text'])\n",
    "    print('\\n--------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    results.to_csv(r'res'+ str(SAMPLE_SIZE) +'.csv', index = False, header=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mc = MasterClassifier(global_params=glps[0])    \n",
    "# classifiers[keys[0]] = [mc]\n",
    "# classifiers[keys[0]][0].load_dataset(train['text'], train['novel'])\n",
    "# classifiers[keys[0]][0].train()\n",
    "#classifiers[keys[0]][0].predict(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table list, in each row, the labels predicetd by the classifiers. The first column contains the true labels of each test instance, the other columns contain the predicted label wrt a particular classifier.\n",
    "Each classifier's name is represented by its specific global-parameters tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>[0.8, 0.01, 0.01, 0.0, 0.0]</th>\n",
       "      <th>[0.8, 0.01, 0.01, 0.0, 0.2]</th>\n",
       "      <th>[0.8, 0.01, 0.01, 0.2, 0.0]</th>\n",
       "      <th>[0.8, 0.01, 0.01, 0.2, 0.2]</th>\n",
       "      <th>[0.4, 0.01, 0.01, 0.0, 0.0]</th>\n",
       "      <th>[0.4, 0.01, 0.01, 0.0, 0.2]</th>\n",
       "      <th>[0.4, 0.01, 0.01, 0.2, 0.0]</th>\n",
       "      <th>[0.4, 0.01, 0.01, 0.2, 0.2]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ground_truth  [0.8, 0.01, 0.01, 0.0, 0.0]  [0.8, 0.01, 0.01, 0.0, 0.2]  \\\n",
       "0               4                            4                            4   \n",
       "1               6                            6                            9   \n",
       "2              11                           11                            1   \n",
       "3               3                           11                            3   \n",
       "4               2                            1                            3   \n",
       "..            ...                          ...                          ...   \n",
       "158            10                            6                           10   \n",
       "159             7                            1                            2   \n",
       "160             1                            6                            1   \n",
       "161             5                            5                            4   \n",
       "162             5                            4                            5   \n",
       "\n",
       "     [0.8, 0.01, 0.01, 0.2, 0.0]  [0.8, 0.01, 0.01, 0.2, 0.2]  \\\n",
       "0                              4                            4   \n",
       "1                              6                            6   \n",
       "2                             10                           10   \n",
       "3                              3                            3   \n",
       "4                              3                            1   \n",
       "..                           ...                          ...   \n",
       "158                            7                           10   \n",
       "159                            6                            7   \n",
       "160                            1                            1   \n",
       "161                            4                            5   \n",
       "162                            8                            5   \n",
       "\n",
       "     [0.4, 0.01, 0.01, 0.0, 0.0]  [0.4, 0.01, 0.01, 0.0, 0.2]  \\\n",
       "0                              4                            4   \n",
       "1                             11                            6   \n",
       "2                             10                            1   \n",
       "3                              3                            3   \n",
       "4                              3                            3   \n",
       "..                           ...                          ...   \n",
       "158                            7                           10   \n",
       "159                            7                            2   \n",
       "160                            1                            6   \n",
       "161                            5                            5   \n",
       "162                            4                            5   \n",
       "\n",
       "     [0.4, 0.01, 0.01, 0.2, 0.0]  [0.4, 0.01, 0.01, 0.2, 0.2]  \n",
       "0                              4                            4  \n",
       "1                              6                            6  \n",
       "2                              6                           10  \n",
       "3                              3                            3  \n",
       "4                              3                            3  \n",
       "..                           ...                          ...  \n",
       "158                            7                           10  \n",
       "159                            7                            7  \n",
       "160                            1                            1  \n",
       "161                            5                            5  \n",
       "162                            5                            5  \n",
       "\n",
       "[163 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has the goal to build the final DataFrame containing all the metrics of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = pd.DataFrame()\n",
    "accuracy = pd.DataFrame()\n",
    "confusion_matrix = []\n",
    "col_sum = []\n",
    "row_sum = []\n",
    "c = 0\n",
    "\n",
    "\n",
    "# accuracy utility computation\n",
    "for name in list(results):\n",
    "    if name != 'ground_truth':\n",
    "        tmp = np.where(results['ground_truth'] == results[name], 1, 0)\n",
    "        accuracy[name] = tmp\n",
    "        confusion_matrix.append(np.zeros((12,12)))        \n",
    "\n",
    "        \n",
    "# confusion matrix computation\n",
    "for name in list(accuracy):\n",
    "    for ins in range(0,len(accuracy)):\n",
    "        confusion_matrix[c][results['ground_truth'][ins]][results[name][ins]] += 1\n",
    "    col_sum.append(confusion_matrix[c].sum(0))\n",
    "    c=c+1\n",
    "row_sum=confusion_matrix[0].sum(1)\n",
    "\n",
    "\n",
    "# TPR & FPR computation\n",
    "tpr = np.zeros((len(list(accuracy)),12))\n",
    "fpr = np.zeros((len(list(accuracy)),12))\n",
    "\n",
    "for cl in range(0, len(list(accuracy))):\n",
    "    for lbl in range(0,12):\n",
    "        if row_sum[lbl] != 0:       \n",
    "            tpr[cl][lbl] = confusion_matrix[cl][lbl][lbl]/row_sum[lbl]\n",
    "        if (row_sum.sum()-row_sum[lbl]) != 0:\n",
    "            fpr[cl][lbl] = (col_sum[cl][lbl]-confusion_matrix[cl][lbl][lbl]) / (row_sum.sum()-row_sum[lbl])\n",
    "\n",
    "            \n",
    "final_res['classifier'] = list(accuracy)\n",
    "#final_res['confusion_matrix'] = confusion_matrix\n",
    "final_res['accuracy'] = accuracy.apply(lambda x: np.mean(x)).tolist()\n",
    "\n",
    "lb = test['novel'].apply(lambda x: np.argmax(x)).unique()\n",
    "lb.sort()\n",
    "\n",
    "for idx in range(0,12):\n",
    "    final_res['TPR'+str(idx)] = [row[idx] for row in tpr]\n",
    "    final_res['FPR'+str(idx)] = [row[idx] for row in fpr]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TPR0</th>\n",
       "      <th>FPR0</th>\n",
       "      <th>TPR1</th>\n",
       "      <th>FPR1</th>\n",
       "      <th>TPR2</th>\n",
       "      <th>FPR2</th>\n",
       "      <th>TPR3</th>\n",
       "      <th>FPR3</th>\n",
       "      <th>...</th>\n",
       "      <th>TPR7</th>\n",
       "      <th>FPR7</th>\n",
       "      <th>TPR8</th>\n",
       "      <th>FPR8</th>\n",
       "      <th>TPR9</th>\n",
       "      <th>FPR9</th>\n",
       "      <th>TPR10</th>\n",
       "      <th>FPR10</th>\n",
       "      <th>TPR11</th>\n",
       "      <th>FPR11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.8, 0.01, 0.01, 0.0, 0.0]</td>\n",
       "      <td>0.453988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.130137</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.8, 0.01, 0.01, 0.0, 0.2]</td>\n",
       "      <td>0.705521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.8, 0.01, 0.01, 0.2, 0.0]</td>\n",
       "      <td>0.515337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.8, 0.01, 0.01, 0.2, 0.2]</td>\n",
       "      <td>0.662577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.4, 0.01, 0.01, 0.0, 0.0]</td>\n",
       "      <td>0.466258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.089041</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.4, 0.01, 0.01, 0.0, 0.2]</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.4, 0.01, 0.01, 0.2, 0.0]</td>\n",
       "      <td>0.460123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.075342</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.4, 0.01, 0.01, 0.2, 0.2]</td>\n",
       "      <td>0.711656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    classifier  accuracy  TPR0      FPR0      TPR1      FPR1  \\\n",
       "0  [0.8, 0.01, 0.01, 0.0, 0.0]  0.453988   0.0  0.000000  0.555556  0.096552   \n",
       "1  [0.8, 0.01, 0.01, 0.0, 0.2]  0.705521   1.0  0.000000  0.777778  0.027586   \n",
       "2  [0.8, 0.01, 0.01, 0.2, 0.0]  0.515337   0.0  0.006173  0.833333  0.096552   \n",
       "3  [0.8, 0.01, 0.01, 0.2, 0.2]  0.662577   0.0  0.006173  0.777778  0.082759   \n",
       "4  [0.4, 0.01, 0.01, 0.0, 0.0]  0.466258   1.0  0.012346  0.555556  0.075862   \n",
       "5  [0.4, 0.01, 0.01, 0.0, 0.2]  0.674847   0.0  0.012346  0.777778  0.048276   \n",
       "6  [0.4, 0.01, 0.01, 0.2, 0.0]  0.460123   0.0  0.000000  0.611111  0.131034   \n",
       "7  [0.4, 0.01, 0.01, 0.2, 0.2]  0.711656   0.0  0.006173  0.777778  0.034483   \n",
       "\n",
       "    TPR2      FPR2      TPR3      FPR3  ...      TPR7      FPR7      TPR8  \\\n",
       "0  0.250  0.032258  0.421053  0.055556  ...  0.470588  0.130137  0.631579   \n",
       "1  0.500  0.012903  0.736842  0.076389  ...  0.705882  0.068493  0.789474   \n",
       "2  0.375  0.019355  0.578947  0.048611  ...  0.705882  0.068493  0.526316   \n",
       "3  0.375  0.006452  0.789474  0.041667  ...  0.705882  0.068493  0.842105   \n",
       "4  0.250  0.012903  0.473684  0.097222  ...  0.647059  0.089041  0.473684   \n",
       "5  0.250  0.019355  0.736842  0.062500  ...  0.823529  0.041096  0.789474   \n",
       "6  0.125  0.019355  0.421053  0.062500  ...  0.705882  0.075342  0.631579   \n",
       "7  0.500  0.025806  0.684211  0.041667  ...  0.823529  0.061644  0.736842   \n",
       "\n",
       "       FPR8  TPR9      FPR9     TPR10     FPR10     TPR11     FPR11  \n",
       "0  0.076389  0.00  0.006289  0.333333  0.041379  0.307692  0.020000  \n",
       "1  0.027778  0.75  0.006289  0.555556  0.020690  0.769231  0.006667  \n",
       "2  0.069444  0.00  0.006289  0.388889  0.020690  0.230769  0.006667  \n",
       "3  0.041667  0.50  0.006289  0.722222  0.048276  0.461538  0.013333  \n",
       "4  0.027778  0.25  0.000000  0.388889  0.075862  0.384615  0.040000  \n",
       "5  0.048611  0.75  0.006289  0.611111  0.041379  0.615385  0.000000  \n",
       "6  0.069444  0.00  0.000000  0.388889  0.055172  0.230769  0.013333  \n",
       "7  0.034722  1.00  0.006289  0.722222  0.041379  0.615385  0.013333  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will print the ROC-GRAPH specific to a particolar class label *roc_l*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for roc_l in range(0,12):\n",
    "    x = final_res.plot(x ='FPR'+str(roc_l), y='TPR'+str(roc_l), kind = 'scatter',figsize=(10,10))\n",
    "    x.axline([0,0],[1,1])    \n",
    "    plt.show()\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
